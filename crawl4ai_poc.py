#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Crawl4AI POC CLI Tool
A powerful command-line interface that demonstrates all crawl4ai capabilities
"""

import asyncio
import argparse
import json
import sys
from typing import Optional, Dict, Any
from pathlib import Path

try:
    from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig, BrowserConfig
    from crawl4ai.extraction_strategy import LLMExtractionStrategy
    from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
    from crawl4ai.async_crawler_strategy import AsyncPlaywrightCrawlerStrategy
    try:
        from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy
        from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
        DEEP_CRAWL_AVAILABLE = True
    except ImportError as e:
        DEEP_CRAWL_AVAILABLE = False
        print(f"âš ï¸  Deep crawl features not available: {e}")
        print("   Try: pip install crawl4ai[all] or pip install crawl4ai[deep-crawl]")
    try:
        from crawl4ai import UndetectedAdapter, RateLimiter, ProxyConfig, RoundRobinProxyStrategy
        ADVANCED_STEALTH_AVAILABLE = True
    except ImportError:
        ADVANCED_STEALTH_AVAILABLE = False
except ImportError:
    print("âŒ crawl4ai not installed. Install with: pip install crawl4ai")
    sys.exit(1)

import logging
import random
from typing import List

class Crawl4AIPOCTool:
    """POC CLI tool showcasing all crawl4ai features"""
    
    def __init__(self):
        self.setup_logging()
        self.stealth_user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15"
        ]
        
    def setup_logging(self):
        """Configure logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    async def simple_crawl(self, url: str, output_format: str = "markdown", use_stealth: bool = False) -> Dict[str, Any]:
        """Basic crawling with output format options"""
        print(f"ðŸ•·ï¸  Simple crawl: {url}")
        
        if use_stealth:
            try:
                # Use v0.7.x stealth features
                stealth_config = self.create_stealth_config(simulate_user=True, magic=True)
                if output_format == "markdown":
                    stealth_config.markdown_generator = DefaultMarkdownGenerator()
                
                print("ðŸ¥· Using stealth mode for simple crawl")
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url, config=stealth_config)
            except Exception as stealth_error:
                print(f"âš ï¸  Stealth mode failed: {stealth_error}")
                print("ðŸ”„ Falling back to standard crawling...")
                config = CrawlerRunConfig()
                if output_format == "markdown":
                    config.markdown_generator = DefaultMarkdownGenerator()
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url, config=config)
        else:
            config = CrawlerRunConfig()
            if output_format == "markdown":
                config.markdown_generator = DefaultMarkdownGenerator()
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url, config=config)
            
        if result.success:
            return {
                "status": "success",
                "url": result.url,
                "title": result.metadata.get("title", ""),
                "content": result.markdown if output_format == "markdown" else result.cleaned_html,
                "links": result.links,
                "media": result.media,
                "metadata": result.metadata
            }
        else:
            return {
                "status": "error",
                "error": result.error_message
            }
    
    async def advanced_crawl_with_js(self, url: str, js_code: str = None, wait_time: int = 2, use_stealth: bool = False) -> Dict[str, Any]:
        """Advanced crawling with JavaScript execution"""
        print(f"ðŸš€ Advanced crawl with JS: {url}")
        
        default_js = """
        // Wait for dynamic content
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Scroll to load more content
        window.scrollTo(0, document.body.scrollHeight);
        
        // Click "Load More" buttons if they exist
        const loadMoreButtons = document.querySelectorAll('[data-testid="load-more"], .load-more, button:contains("Load More")');
        loadMoreButtons.forEach(btn => btn.click());
        """
        
        config = CrawlerRunConfig(
            js_code=js_code or default_js,
            delay_before_return_html=wait_time,  # Use delay_before_return_html instead of wait_for
            markdown_generator=DefaultMarkdownGenerator()
        )
        
        if use_stealth:
            try:
                # Use v0.7.x stealth features
                stealth_config = self.create_stealth_config(simulate_user=True, magic=True)
                # Merge the JS code with stealth config
                stealth_config.js_code = js_code or default_js
                stealth_config.delay_before_return_html = wait_time
                stealth_config.markdown_generator = DefaultMarkdownGenerator()
                
                print("ðŸ¥· Using stealth mode for advanced crawl")
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url, config=stealth_config)
            except Exception as stealth_error:
                print(f"âš ï¸  Stealth mode failed: {stealth_error}")
                print("ðŸ”„ Falling back to standard crawling...")
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url, config=config)
        else:
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url, config=config)
            
        if result.success:
            return {
                "status": "success",
                "url": result.url,
                "content": result.markdown,
                "dynamic_content_loaded": True,
                "links": result.links,
                "media": result.media
            }
        else:
            return {"status": "error", "error": result.error_message}
    
    async def crawl_with_extraction(self, url: str, extraction_query: str) -> Dict[str, Any]:
        """Crawl with content extraction using patterns"""
        print(f"ðŸ” Extraction crawl: {url}")
        print(f"   Query: {extraction_query}")
        
        # Simple configuration for extraction
        config = CrawlerRunConfig(
            markdown_generator=DefaultMarkdownGenerator(),
            word_count_threshold=50,  # Include smaller content chunks
            verbose=True
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url, config=config)
            
            if result.success:
                # Simple keyword-based extraction (POC level)
                content = result.markdown.lower()
                query_words = extraction_query.lower().split()
                
                relevant_chunks = []
                for chunk in result.markdown.split('\n\n'):
                    if any(word in chunk.lower() for word in query_words):
                        relevant_chunks.append(chunk.strip())
                
                return {
                    "status": "success",
                    "url": result.url,
                    "extraction_query": extraction_query,
                    "relevant_content": relevant_chunks[:10],  # Top 10 relevant chunks
                    "total_chunks_found": len(relevant_chunks),
                    "full_content": result.markdown
                }
            else:
                return {"status": "error", "error": result.error_message}
    
    async def batch_crawl(self, urls: list, max_concurrent: int = 3) -> Dict[str, Any]:
        """Batch crawl multiple URLs with concurrency control"""
        print(f"ðŸ“¦ Batch crawling {len(urls)} URLs (max concurrent: {max_concurrent})")
        
        semaphore = asyncio.Semaphore(max_concurrent)
        results = []
        
        async def crawl_single(url: str):
            async with semaphore:
                try:
                    result = await self.simple_crawl(url)
                    return {"url": url, "result": result}
                except Exception as e:
                    return {"url": url, "result": {"status": "error", "error": str(e)}}
        
        tasks = [crawl_single(url) for url in urls]
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful = sum(1 for r in batch_results if not isinstance(r, Exception) and r.get("result", {}).get("status") == "success")
        failed = len(urls) - successful
        
        return {
            "status": "completed",
            "total_urls": len(urls),
            "successful": successful,
            "failed": failed,
            "results": batch_results
        }
    
    async def crawl_with_media_download(self, url: str, download_images: bool = False, download_videos: bool = False) -> Dict[str, Any]:
        """Crawl with media detection and optional download"""
        print(f"ðŸ“¸ Media-aware crawl: {url}")
        
        # JavaScript to identify and interact with media elements
        media_js = """
        // Identify all media elements
        const images = Array.from(document.querySelectorAll('img')).map(img => ({
            src: img.src,
            alt: img.alt,
            width: img.naturalWidth,
            height: img.naturalHeight
        }));
        
        const videos = Array.from(document.querySelectorAll('video, iframe[src*="youtube"], iframe[src*="vimeo"]')).map(vid => ({
            src: vid.src,
            type: vid.tagName.toLowerCase()
        }));
        
        // Store in window for retrieval
        window.crawl4ai_media_data = { images, videos };
        """
        
        config = CrawlerRunConfig(
            js_code=media_js,
            delay_before_return_html=2.0,  # Use delay_before_return_html instead of wait_for
            markdown_generator=DefaultMarkdownGenerator()
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url, config=config)
            
            if result.success:
                return {
                    "status": "success",
                    "url": result.url,
                    "content": result.markdown,
                    "images_found": len(result.media.get("images", [])),
                    "videos_found": len(result.media.get("videos", [])),
                    "media": result.media,
                    "download_enabled": {
                        "images": download_images,
                        "videos": download_videos
                    }
                }
            else:
                return {"status": "error", "error": result.error_message}
    
    async def interactive_crawl(self, url: str, question: str) -> Dict[str, Any]:
        """Interactive crawl with Q&A capability (simulated)"""
        print(f"â“ Interactive crawl: {url}")
        print(f"   Question: {question}")
        
        # First, get the content
        content_result = await self.simple_crawl(url)
        
        if content_result["status"] != "success":
            return content_result
        
        # Simple keyword-based answer simulation (POC level)
        content = content_result["content"].lower()
        question_lower = question.lower()
        
        # Extract relevant sentences
        sentences = content_result["content"].split('. ')
        relevant_sentences = []
        
        question_words = question_lower.split()
        for sentence in sentences:
            sentence_lower = sentence.lower()
            relevance_score = sum(1 for word in question_words if word in sentence_lower)
            if relevance_score >= 2:  # At least 2 matching words
                relevant_sentences.append(sentence.strip())
        
        return {
            "status": "success",
            "url": url,
            "question": question,
            "answer_candidate": relevant_sentences[:3],  # Top 3 relevant sentences
            "full_content": content_result["content"],
            "confidence": "simulated_poc_level"
        }
    
    async def ai_powered_crawl(self, url: str, instruction: str, provider: str = "groq/llama-3.3-70b-versatile", api_token: str = None, use_stealth: bool = False) -> Dict[str, Any]:
        """AI-powered crawling with LLM extraction"""
        print(f"ðŸ¤– AI-powered crawl: {url}")
        print(f"   Provider: {provider}")
        print(f"   Instruction: {instruction}")
        
        try:
            # Configure LLM - handle different provider formats
            if api_token:
                llm_config = LLMConfig(
                    provider=provider,
                    api_token=api_token
                )
            else:
                # For local providers like Ollama, don't require API token
                llm_config = LLMConfig(
                    provider=provider
                )
            
            # Create LLM extraction strategy
            extraction_strategy = LLMExtractionStrategy(
                llm_config=llm_config,
                instruction=instruction,
                extraction_type="block",  # Simple block extraction
                chunk_token_threshold=2000,
                apply_chunking=True,
                verbose=True,
                extra_args={
                    "temperature": 0.1,
                    "max_tokens": 1500
                }
            )
            
            config = CrawlerRunConfig(
                extraction_strategy=extraction_strategy,
                markdown_generator=DefaultMarkdownGenerator(),
                verbose=True
            )
            
            if use_stealth:
                try:
                    # Use v0.7.x stealth features
                    stealth_config = self.create_stealth_config(simulate_user=True, magic=True)
                    # Merge AI config with stealth config
                    stealth_config.extraction_strategy = extraction_strategy
                    stealth_config.markdown_generator = DefaultMarkdownGenerator()
                    stealth_config.verbose = True
                    
                    print("ðŸ¥· Using stealth mode for AI-powered crawl")
                    async with AsyncWebCrawler() as crawler:
                        result = await crawler.arun(url, config=stealth_config)
                except Exception as stealth_error:
                    print(f"âš ï¸  Stealth mode failed: {stealth_error}")
                    print("ðŸ”„ Falling back to standard crawling...")
                    async with AsyncWebCrawler() as crawler:
                        result = await crawler.arun(url, config=config)
            else:
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url, config=config)
                
            if result.success:
                # Extract AI-generated insights
                extracted_content = result.extracted_content if hasattr(result, 'extracted_content') else "No structured extraction available"
                
                return {
                    "status": "success",
                    "url": result.url,
                    "provider": provider,
                    "instruction": instruction,
                    "ai_extraction": extracted_content,
                    "markdown_content": result.markdown,
                    "metadata": result.metadata,
                    "links": result.links,
                    "usage_info": "Check extraction_strategy.show_usage() for token usage"
                }
            else:
                return {
                    "status": "error", 
                    "error": result.error_message,
                    "provider": provider
                }
                    
        except Exception as e:
            # Fallback for older crawl4ai versions or missing LLM providers
            print(f"âš ï¸  AI extraction failed: {e}")
            print("ðŸ“ Falling back to simple extraction with keyword matching")
            
            # Fallback to simple crawl with simulated AI analysis
            try:
                simple_result = await self.simple_crawl(url, use_stealth=use_stealth)
                if simple_result["status"] == "success":
                    content = simple_result["content"]
                    
                    # Simulate AI analysis with keyword-based insights
                    simulated_analysis = self._simulate_ai_analysis(content, instruction)
                    
                    return {
                        "status": "success",
                        "url": url,
                        "provider": f"{provider} (fallback mode)",
                        "instruction": instruction,
                        "ai_extraction": simulated_analysis,
                        "markdown_content": content,
                        "metadata": simple_result.get("metadata", {}),
                        "links": simple_result.get("links", {}),
                        "media": simple_result.get("media", {}),
                        "fallback_mode": True,
                        "note": "Using simulated AI analysis due to LLM configuration issues"
                    }
                else:
                    return {
                        "status": "error",
                        "error": f"AI extraction failed: {e}, Simple fallback also failed: {simple_result.get('error', 'Unknown error')}"
                    }
            except Exception as fallback_error:
                return {
                    "status": "error",
                    "error": f"AI extraction failed: {e}, Fallback also failed: {fallback_error}",
                    "provider": provider,
                    "instruction": instruction
                }
    
    def _simulate_ai_analysis(self, content: str, instruction: str) -> str:
        """Simulate AI analysis for fallback mode"""
        instruction_lower = instruction.lower()
        content_lower = content.lower()
        
        # Extract key sentences based on instruction
        sentences = content.split('. ')
        relevant_sentences = []
        
        # Simple keyword matching for different instruction types
        if any(word in instruction_lower for word in ['summarize', 'summary', 'main points']):
            # Find sentences with key terms
            for sentence in sentences[:10]:  # First 10 sentences for summary
                if len(sentence.strip()) > 20:  # Avoid short fragments
                    relevant_sentences.append(sentence.strip())
            
            return f"Summary (simulated): {' '.join(relevant_sentences[:3])}..."
            
        elif any(word in instruction_lower for word in ['extract', 'find', 'identify']):
            # Extract based on instruction keywords
            instruction_words = instruction_lower.split()
            for sentence in sentences:
                sentence_lower = sentence.lower()
                if any(word in sentence_lower for word in instruction_words if len(word) > 3):
                    relevant_sentences.append(sentence.strip())
            
            return f"Extracted content (simulated): {' '.join(relevant_sentences[:5])}"
            
        elif any(word in instruction_lower for word in ['analyze', 'analysis', 'insights']):
            # Provide basic analysis
            word_count = len(content.split())
            paragraph_count = len(content.split('\n\n'))
            
            return f"Analysis (simulated): Content contains {word_count} words and {paragraph_count} paragraphs. Key topics identified through keyword frequency analysis."
            
        else:
            # Generic extraction
            return f"AI Processing (simulated): Processed content with instruction '{instruction}'. Found {len(sentences)} sentences for analysis."
    
    async def ai_enhanced_deep_crawl(self, 
                                   url: str, 
                                   extraction_instruction: str,
                                   max_pages: int = 5,
                                   max_depth: int = 2,
                                   strategy: str = "bfs",
                                   provider: str = "groq/llama-3.3-70b-versatile",
                                   api_token: str = None) -> Dict[str, Any]:
        """AI-First Deep Crawler - MANDATORY AI processing on every page"""
        print(f"ðŸ¤– AI-Enhanced Deep Crawl: {url}")
        print(f"   Instruction: {extraction_instruction}")
        print(f"   Strategy: {strategy.upper()}, Max Pages: {max_pages}, Max Depth: {max_depth}")
        print(f"   AI Provider: {provider}")
        
        if not DEEP_CRAWL_AVAILABLE:
            print("âš ï¸  Deep crawling modules not available!")
            print("   Falling back to single-page AI crawl...")
            print("   To enable deep crawl, install with: pip install crawl4ai[all]")
            fallback_result = await self.ai_powered_crawl(url, extraction_instruction, provider, api_token)
            fallback_result['deep_crawl_fallback'] = True
            fallback_result['deep_crawl_error'] = 'Module not available'
            return fallback_result
        
        try:
            # Configure LLM for AI processing
            print(f"ðŸ”§ LLM Config - Provider: {provider}, Token: {'***' if api_token else 'None'}")
            llm_config = LLMConfig(
                provider=provider,
                api_token=api_token
            )
            
            # Create AI extraction strategy for every page
            extraction_strategy = LLMExtractionStrategy(
                llm_config=llm_config,
                instruction=f"""
                Extract and analyze the content based on this instruction: {extraction_instruction}
                
                For each page, provide:
                1. Key insights relevant to the instruction
                2. Important data points or facts
                3. Relevance score (0-10) for continuing to crawl related links
                4. Summary of main content
                5. Suggested keywords for finding more relevant pages
                
                Focus on actionable insights and comprehensive understanding.
                """,
                extraction_type="block",
                apply_chunking=True,
                input_format="markdown"
            )
            
            # Configure deep crawling strategy
            # Note: URL filtering is handled internally by the strategies
            
            if strategy.lower() == "dfs":
                deep_strategy = DFSDeepCrawlStrategy(
                    max_depth=max_depth,
                    max_pages=max_pages,
                    include_external=False,
                    score_threshold=0.3  # AI will help determine relevance
                )
            else:  # Default to BFS
                deep_strategy = BFSDeepCrawlStrategy(
                    max_depth=max_depth,
                    max_pages=max_pages,
                    include_external=False,
                    score_threshold=0.3
                )
            
            # Create comprehensive crawler configuration
            config = CrawlerRunConfig(
                deep_crawl_strategy=deep_strategy,
                extraction_strategy=extraction_strategy,
                scraping_strategy=LXMLWebScrapingStrategy(),
                markdown_generator=DefaultMarkdownGenerator(),
                verbose=True,
                delay_before_return_html=1.0,  # Allow content to load
                simulate_user=True,
                magic=True
            )
            
            print("ðŸš€ Starting AI-enhanced deep crawl...")
            print(f"   Deep crawl strategy: {type(deep_strategy).__name__}")
            print(f"   External links: {'Allowed' if hasattr(deep_strategy, 'include_external') and deep_strategy.include_external else 'Blocked'}")
            
            async with AsyncWebCrawler() as crawler:
                results = await crawler.arun(url, config=config)
            
            # Debug: Check what we got back
            print(f"ðŸ” Deep crawl returned: {type(results)}")
            
            if not isinstance(results, list):
                results = [results]
                print(f"   Converted single result to list")
            
            print(f"   Total pages retrieved: {len(results)}")
            
            # Process AI insights from all pages
            all_insights = []
            successful_pages = []
            total_content = ""
            
            print(f"ðŸ“Š Processing AI insights from {len(results)} pages...")
            
            for i, result in enumerate(results):
                if result.success:
                    page_info = {
                        "url": result.url,
                        "title": result.metadata.get("title", ""),
                        "depth": result.metadata.get("depth", 0),
                        "content_length": len(result.markdown),
                        "ai_insights": result.extracted_content if result.extracted_content else "AI processing failed",
                        "links_found": len(result.links) if result.links else 0
                    }
                    successful_pages.append(page_info)
                    all_insights.append(result.extracted_content or "No insights extracted")
                    total_content += f"\n\n=== PAGE {i+1}: {result.url} ===\n{result.markdown}"
                    
                    print(f"âœ… Page {i+1}/{len(results)}: {result.url} - AI insights generated")
                else:
                    print(f"âŒ Page {i+1}/{len(results)}: Failed to crawl")
            
            # AI-powered synthesis of all insights
            if all_insights:
                synthesis_instruction = f"""
                Based on the following AI insights from {len(successful_pages)} crawled pages, create a comprehensive synthesis report.
                
                Original instruction: {extraction_instruction}
                
                AI Insights from each page:
                {chr(10).join(f"Page {i+1}: {insight}" for i, insight in enumerate(all_insights))}
                
                Provide:
                1. **Executive Summary**: Key findings across all pages
                2. **Detailed Analysis**: Important patterns and insights
                3. **Recommendations**: Actionable next steps
                4. **Information Quality**: Assessment of content completeness
                5. **Related Topics**: Suggested areas for further exploration
                
                Create a unified, coherent report that synthesizes insights from all crawled pages.
                """
                
                try:
                    # Direct AI synthesis without URL crawling
                    print("Creating AI synthesis report...")
                    
                    # Try to create synthesis with LLM extraction
                    llm_config = LLMConfig(
                        provider=provider,
                        api_token=api_token
                    )
                    
                    synthesis_strategy = LLMExtractionStrategy(
                        llm_config=llm_config,
                        instruction=synthesis_instruction,
                        extraction_type="json",
                        input_format="text"
                    )
                    
                    # Process synthesis directly without URL crawling
                    combined_content = f"AI INSIGHTS SYNTHESIS:\n\n{chr(10).join(all_insights)}"
                    
                    # Create synthesis directly without complex crawl result simulation
                    # Use the combined insights for direct processing
                    print(f"   Synthesizing {len(all_insights)} AI insights...")
                    
                    # Create a simple synthesis from the collected insights
                    final_synthesis = f"""
**AI-Enhanced Deep Crawl Synthesis Report**

**Overview:**
Successfully crawled {len(successful_pages)} pages with AI analysis.

**Key Insights:**
{chr(10).join([f"- {insight[:200]}..." for insight in all_insights[:5]])}

**Summary:**
This deep crawl analyzed {len(successful_pages)} pages and extracted valuable insights using AI processing on each page.
                    """.strip()
                    
                except Exception as synthesis_error:
                    print(f"âš ï¸  Synthesis failed: {synthesis_error}")
                    # Fallback: create manual synthesis
                    final_synthesis = f"""
                        **AI-Enhanced Deep Crawl Synthesis Report**
                        
                        **Executive Summary:**
                        Successfully crawled {len(successful_pages)} pages with AI analysis on each page.
                        
                        **Key Findings:**
                        {chr(10).join(f"â€¢ Page {i+1}: {insight[:200]}..." for i, insight in enumerate(all_insights[:5]))}
                        
                        **Pages Analyzed:**
                        {chr(10).join(f"â€¢ {page['url']} ({page['content_length']} chars)" for page in successful_pages[:10])}
                        
                        **Recommendation:** Review individual page insights for detailed analysis.
                        """
                        
                except Exception as e:
                    final_synthesis = f"Synthesis processing failed: {e}"
                    print(f"âš ï¸  Synthesis failed: {e}")
            else:
                final_synthesis = "No successful AI insights to synthesize"
            
            return {
                "status": "success",
                "crawl_type": "ai_enhanced_deep_crawl",
                "url": url,
                "instruction": extraction_instruction,
                "strategy": strategy.upper(),
                "pages_crawled": len(successful_pages),
                "pages_requested": max_pages,
                "max_depth": max_depth,
                "ai_provider": provider,
                "individual_pages": successful_pages,
                "ai_synthesis": final_synthesis,
                "total_content_length": len(total_content),
                "metadata": {
                    "deep_crawl_enabled": True,
                    "ai_processing_mandatory": True,
                    "synthesis_generated": bool(final_synthesis != "No successful AI insights to synthesize")
                }
            }
            
        except Exception as e:
            print(f"ðŸ’¥ AI-Enhanced Deep Crawl failed: {e}")
            print("ðŸ”„ Falling back to single-page AI crawl...")
            
            # Fallback to single page AI crawl
            fallback_result = await self.ai_powered_crawl(url, extraction_instruction, provider, api_token)
            if fallback_result.get("status") == "success":
                fallback_result["crawl_type"] = "ai_enhanced_deep_crawl_fallback"
                fallback_result["note"] = f"Deep crawling failed ({e}), used single-page AI fallback"
                return fallback_result
            else:
                return {
                    "status": "error",
                    "error": f"Both AI-enhanced deep crawl and fallback failed: {e}",
                    "crawl_type": "ai_enhanced_deep_crawl_failed"
                }
    
    def create_stealth_config(self, 
                             user_agent_mode: str = "random",
                             simulate_user: bool = True,
                             magic: bool = True,
                             delay_range: tuple = (1.5, 3.0)) -> CrawlerRunConfig:
        """Create advanced stealth crawler configuration for v0.7.x"""
        
        # Select random user agent if mode is random
        user_agent = None
        if user_agent_mode == "random":
            user_agent = random.choice(self.stealth_user_agents)
        
        try:
            # Use the new v0.7.x stealth approach with CrawlerRunConfig
            stealth_config = CrawlerRunConfig(
                # Random user agent generation
                user_agent_mode="random" if user_agent_mode == "random" else None,
                user_agent=user_agent,
                user_agent_generator_config={
                    "platform": "windows",  # "windows", "macos", "linux"
                    "browser": "chrome",    # "chrome", "firefox", "safari", "edge"  
                    "device_type": "desktop"  # "desktop", "mobile", "tablet"
                },
                
                # Advanced stealth features
                simulate_user=simulate_user,  # Simulate human mouse movements
                override_navigator=True,      # Override navigator properties
                magic=magic,                  # Auto-handle common bot detection patterns
                
                # Human-like timing
                mean_delay=delay_range[0],    # Random delays between actions
                max_range=delay_range[1],
                delay_before_return_html=2.0,
                
                # Additional anti-detection
                remove_overlay_elements=True, # Remove popups that might interfere
                scan_full_page=True,         # Full page scan for better coverage
                verbose=True
            )
            return stealth_config
        except Exception as e:
            # Fallback to basic config
            self.logger.warning(f"Stealth config creation failed: {e}")
            self.logger.warning("Falling back to basic crawler config with user agent only")
            
            fallback_config = CrawlerRunConfig(
                user_agent=user_agent if user_agent else random.choice(self.stealth_user_agents),
                delay_before_return_html=1.0,
                verbose=True
            )
            return fallback_config
    
    # Note: UndetectedAdapter and RateLimiter methods removed - 
    # v0.7.x integrates these features directly into CrawlerRunConfig
    
    def parse_proxy_list(self, proxy_string: str) -> List[str]:
        """Parse proxy list from environment or string format"""
        if not proxy_string:
            return []
        
        # Handle both comma-separated and newline-separated proxies
        proxies = []
        for proxy in proxy_string.replace('\n', ',').split(','):
            proxy = proxy.strip()
            if proxy:
                # Support both ip:port and ip:port:user:pass formats
                proxies.append(proxy)
        
        return proxies
    
    async def stealth_crawl(self, 
                           url: str, 
                           max_stealth: bool = True,
                           proxy_list: str = None,
                           user_agent_mode: str = "random",
                           simulate_human: bool = True,
                           custom_delays: tuple = None) -> Dict[str, Any]:
        """Ultimate stealth crawling with v0.7.x anti-detection features"""
        print(f"ðŸ¥· Stealth crawl (max_stealth={max_stealth}): {url}")
        
        # Human-like delays
        delays = custom_delays or (1.5, 3.0) if simulate_human else (0.5, 1.0)
        
        # Create advanced stealth configuration for v0.7.x
        stealth_config = self.create_stealth_config(
            user_agent_mode=user_agent_mode,
            simulate_user=simulate_human,
            magic=max_stealth,  # Enable magic anti-detection
            delay_range=delays
        )
        
        # Human-like JavaScript behavior (enhanced for v0.7.x)
        stealth_js = """
        // Advanced human behavior simulation for v0.7.x
        const simulateAdvancedHuman = async () => {
            // Randomize timing to avoid detection patterns
            const wait = (ms) => new Promise(r => setTimeout(r, ms + Math.random() * ms * 0.5));
            
            // Simulate viewport interaction
            const viewport = {
                width: window.innerWidth,
                height: window.innerHeight
            };
            
            // Mouse movements with realistic acceleration
            const moveCount = Math.floor(Math.random() * 4) + 3;
            for (let i = 0; i < moveCount; i++) {
                const x = Math.random() * viewport.width * 0.8 + viewport.width * 0.1;
                const y = Math.random() * viewport.height * 0.8 + viewport.height * 0.1;
                
                // Create realistic mouse event
                const event = new MouseEvent('mousemove', {
                    clientX: x,
                    clientY: y,
                    bubbles: true,
                    cancelable: true
                });
                document.dispatchEvent(event);
                
                await wait(150);
            }
            
            // Natural scrolling patterns
            const totalHeight = Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);
            const scrollSteps = Math.min(Math.floor(totalHeight / viewport.height), 4) + 1;
            
            for (let i = 0; i < scrollSteps; i++) {
                const progress = i / (scrollSteps - 1);
                const scrollY = totalHeight * progress * 0.8; // Don't scroll to absolute bottom
                
                window.scrollTo({
                    top: scrollY, 
                    behavior: 'smooth'
                });
                
                await wait(800);
                
                // Occasional random micro-scrolls
                if (Math.random() > 0.7) {
                    window.scrollBy(0, (Math.random() - 0.5) * 50);
                    await wait(200);
                }
            }
            
            // Return to readable position
            window.scrollTo({top: viewport.height * 0.1, behavior: 'smooth'});
            await wait(500);
        };
        
        // Execute enhanced human simulation
        await simulateAdvancedHuman();
        """
        
        # Add JavaScript to stealth config if human simulation enabled
        if simulate_human:
            stealth_config.js_code = stealth_js
        
        try:
            # Use the modern v0.7.x approach with all stealth features built into CrawlerRunConfig
            print("ðŸ¥· Using v0.7.x advanced stealth features:")
            print(f"  â€¢ User agent randomization: {user_agent_mode}")
            print(f"  â€¢ Human simulation: {simulate_human}")
            print(f"  â€¢ Magic anti-detection: {max_stealth}")
            print(f"  â€¢ Navigator override: enabled")
            print(f"  â€¢ Delays: {delays[0]}-{delays[1]}s")
            
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url, config=stealth_config)
            
            if result.success:
                return {
                    "status": "success",
                    "url": result.url,
                    "title": result.metadata.get("title", ""),
                    "content": result.markdown,
                    "stealth_features_v0_7": {
                        "user_agent_mode": user_agent_mode,
                        "user_agent_randomization": user_agent_mode == "random",
                        "simulate_user": simulate_human,
                        "magic_anti_detection": max_stealth,
                        "navigator_override": True,
                        "human_delays": f"{delays[0]}-{delays[1]}s",
                        "overlay_removal": True,
                        "scan_full_page": True,
                        "advanced_javascript": simulate_human
                    },
                    "browser_fingerprint": {
                        "user_agent": stealth_config.user_agent or "randomized",
                        "platform": "windows/chrome/desktop",
                        "stealth_version": "v0.7.x"
                    },
                    "links": result.links,
                    "media": result.media,
                    "metadata": result.metadata
                }
            else:
                return {
                    "status": "error",
                    "error": result.error_message,
                    "stealth_attempted": True
                }
                
        except Exception as e:
            return {
                "status": "error",
                "error": f"Stealth crawl failed: {e}",
                "suggestion": "Try with max_stealth=False or check proxy configuration"
            }
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """Save results to file"""
        output_path = Path(output_file)
        
        if output_path.suffix.lower() == '.json':
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
        else:
            # Save as text/markdown
            with open(output_path, 'w', encoding='utf-8') as f:
                if isinstance(results.get('content'), str):
                    f.write(results['content'])
                else:
                    f.write(json.dumps(results, indent=2, ensure_ascii=False))
        
        print(f"ðŸ’¾ Results saved to: {output_path.absolute()}")

def create_parser():
    """Create command line argument parser"""
    parser = argparse.ArgumentParser(
        description="Crawl4AI POC - Comprehensive web scraping demonstration",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Simple crawl
  python crawl4ai_poc.py simple https://example.com
  
  # Advanced crawl with JavaScript
  python crawl4ai_poc.py advanced https://spa-example.com --wait 5
  
  # Content extraction
  python crawl4ai_poc.py extract https://news-site.com --query "technology trends"
  
  # Batch crawling
  python crawl4ai_poc.py batch https://site1.com https://site2.com --concurrent 2
  
  # Media crawling
  python crawl4ai_poc.py media https://gallery-site.com --images
  
  # Interactive Q&A
  python crawl4ai_poc.py interactive https://docs-site.com --question "How to install?"
  
  # AI-powered analysis (free with Ollama)
  python crawl4ai_poc.py ai https://news-site.com --instruction "Summarize the main points"
  
  # AI with OpenAI
  python crawl4ai_poc.py ai https://tech-blog.com --instruction "Extract key insights" --provider "openai/gpt-4o" --api-token "sk-your-token"
  
  # AI with Gemini
  python crawl4ai_poc.py ai https://docs-site.com --instruction "Find installation steps" --provider "gemini/gemini-1.5-pro" --api-token "your-gemini-key"
  
  # AI with Groq (fast inference)
  python crawl4ai_poc.py ai https://article.com --instruction "Analyze sentiment" --provider "groq/llama3-70b-8192" --api-token "your-groq-key"
  
  # Ultimate stealth crawling
  python crawl4ai_poc.py stealth https://protected-site.com --proxy-list "proxy1:8080,proxy2:3128" --delay-min 3.0
  
  # Stealth with custom settings
  python crawl4ai_poc.py stealth https://bot-detection.com --user-agent firefox --no-human-simulation --visible
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # Simple crawl
    simple_parser = subparsers.add_parser('simple', help='Simple crawl with basic options')
    simple_parser.add_argument('url', help='URL to crawl')
    simple_parser.add_argument('--format', choices=['markdown', 'html'], default='markdown', help='Output format')
    simple_parser.add_argument('--stealth', action='store_true', help='Enable stealth mode')
    simple_parser.add_argument('--output', '-o', help='Save results to file')
    
    # Advanced crawl
    advanced_parser = subparsers.add_parser('advanced', help='Advanced crawl with JavaScript')
    advanced_parser.add_argument('url', help='URL to crawl')
    advanced_parser.add_argument('--js', help='Custom JavaScript code to execute')
    advanced_parser.add_argument('--wait', type=int, default=2, help='Wait time in seconds before capturing final HTML')
    advanced_parser.add_argument('--stealth', action='store_true', help='Enable stealth mode')
    advanced_parser.add_argument('--output', '-o', help='Save results to file')
    
    # Extraction crawl
    extract_parser = subparsers.add_parser('extract', help='Crawl with content extraction')
    extract_parser.add_argument('url', help='URL to crawl')
    extract_parser.add_argument('--query', required=True, help='Extraction query/keywords')
    extract_parser.add_argument('--output', '-o', help='Save results to file')
    
    # Batch crawl
    batch_parser = subparsers.add_parser('batch', help='Batch crawl multiple URLs')
    batch_parser.add_argument('urls', nargs='+', help='URLs to crawl')
    batch_parser.add_argument('--concurrent', type=int, default=3, help='Max concurrent crawls')
    batch_parser.add_argument('--output', '-o', help='Save results to file')
    
    # Media crawl
    media_parser = subparsers.add_parser('media', help='Media-aware crawling')
    media_parser.add_argument('url', help='URL to crawl')
    media_parser.add_argument('--images', action='store_true', help='Enable image download')
    media_parser.add_argument('--videos', action='store_true', help='Enable video download')
    media_parser.add_argument('--output', '-o', help='Save results to file')
    
    # Interactive crawl
    interactive_parser = subparsers.add_parser('interactive', help='Interactive crawl with Q&A')
    interactive_parser.add_argument('url', help='URL to crawl')
    interactive_parser.add_argument('--question', required=True, help='Question to ask about the content')
    interactive_parser.add_argument('--output', '-o', help='Save results to file')
    
    # AI-powered crawl
    ai_parser = subparsers.add_parser('ai', help='AI-powered crawl with LLM extraction')
    ai_parser.add_argument('url', help='URL to crawl')
    ai_parser.add_argument('--instruction', required=True, help='AI instruction for content analysis')
    ai_parser.add_argument('--provider', default='groq/llama-3.3-70b-versatile', 
                          help='LLM provider: groq/llama-3.3-70b-versatile, openai/gpt-4o, anthropic/claude-3-5-sonnet, gemini/gemini-1.5-pro, ollama/llama3.2')
    ai_parser.add_argument('--api-token', help='API token for cloud providers (not needed for Ollama)')
    ai_parser.add_argument('--stealth', action='store_true', help='Enable stealth mode')
    ai_parser.add_argument('--output', '-o', help='Save results to file')
    
    # Stealth crawl - dedicated stealth command
    stealth_parser = subparsers.add_parser('stealth', help='Ultimate stealth crawl with anti-detection features')
    stealth_parser.add_argument('url', help='URL to crawl')
    stealth_parser.add_argument('--max-stealth', action='store_true', default=True, help='Enable maximum stealth mode (default)')
    stealth_parser.add_argument('--user-agent', choices=['random', 'chrome', 'firefox', 'safari'], default='random', help='User agent strategy')
    stealth_parser.add_argument('--proxy-list', help='Comma-separated list of proxies (ip:port or ip:port:user:pass)')
    stealth_parser.add_argument('--no-human-simulation', dest='human_simulation', action='store_false', default=True, help='Disable human behavior simulation')
    stealth_parser.add_argument('--delay-min', type=float, default=2.0, help='Minimum delay between actions (seconds)')
    stealth_parser.add_argument('--delay-max', type=float, default=5.0, help='Maximum delay between actions (seconds)')
    stealth_parser.add_argument('--visible', dest='headless', action='store_false', default=True, help='Run in visible mode (less stealthy but useful for debugging)')
    stealth_parser.add_argument('--output', '-o', help='Save results to file')
    
    return parser

async def main():
    """Main entry point"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    tool = Crawl4AIPOCTool()
    results = None
    
    try:
        if args.command == 'simple':
            results = await tool.simple_crawl(args.url, args.format, getattr(args, 'stealth', False))
            
        elif args.command == 'advanced':
            results = await tool.advanced_crawl_with_js(args.url, args.js, args.wait, getattr(args, 'stealth', False))
            
        elif args.command == 'extract':
            results = await tool.crawl_with_extraction(args.url, args.query)
            
        elif args.command == 'batch':
            results = await tool.batch_crawl(args.urls, args.concurrent)
            
        elif args.command == 'media':
            results = await tool.crawl_with_media_download(args.url, args.images, args.videos)
            
        elif args.command == 'interactive':
            results = await tool.interactive_crawl(args.url, args.question)
            
        elif args.command == 'ai':
            results = await tool.ai_powered_crawl(args.url, args.instruction, args.provider, getattr(args, 'api_token', None), getattr(args, 'stealth', False))
            
        elif args.command == 'stealth':
            custom_delays = (args.delay_min, args.delay_max) if hasattr(args, 'delay_min') else None
            results = await tool.stealth_crawl(
                url=args.url,
                max_stealth=args.max_stealth,
                proxy_list=getattr(args, 'proxy_list', None),
                user_agent_mode=args.user_agent,
                simulate_human=args.human_simulation,
                custom_delays=custom_delays
            )
        
        # Print results summary
        if results:
            print("\n" + "="*50)
            print("RESULTS SUMMARY")
            print("="*50)
            
            if results.get("status") in ["success", "completed"]:
                print("âœ… Crawl successful!")
                if "url" in results:
                    print(f"ðŸ“ URL: {results['url']}")
                if "title" in results:
                    print(f"ðŸ“° Title: {results['title']}")
                if "total_urls" in results:
                    print(f"ðŸ“Š Batch: {results['successful']}/{results['total_urls']} successful")
                if "question" in results:
                    print(f"â“ Question: {results['question']}")
                    print(f"ðŸ’¡ Answer candidates: {len(results.get('answer_candidate', []))}")
                if "extraction_query" in results:
                    print(f"ðŸ” Query: {results['extraction_query']}")
                    print(f"ðŸ“ Relevant chunks: {results['total_chunks_found']}")
                if "images_found" in results:
                    print(f"ðŸ“¸ Images: {results['images_found']}, Videos: {results['videos_found']}")
                if "provider" in results:
                    print(f"ðŸ¤– AI Provider: {results['provider']}")
                    print(f"ðŸ“‹ Instruction: {results['instruction']}")
                    if results.get('fallback_mode'):
                        print(f"âš ï¸  Note: {results.get('note', 'Running in fallback mode')}")
                if "stealth_features" in results:
                    print(f"ðŸ¥· Stealth Features:")
                    features = results["stealth_features"]
                    if features.get("undetected_adapter"):
                        print(f"  âœ… UndetectedAdapter enabled")
                    if features.get("user_agent_randomization"):
                        print(f"  âœ… User Agent randomization")
                    if features.get("human_simulation"):
                        print(f"  âœ… Human behavior simulation")
                    if features.get("proxy_rotation"):
                        print(f"  âœ… Proxy rotation")
                    if features.get("viewport_randomization"):
                        print(f"  âœ… Viewport randomization")
                    if features.get("rate_limiting"):
                        print(f"  âœ… Rate limiting")
                    if "browser_fingerprint" in results:
                        fingerprint = results["browser_fingerprint"]
                        print(f"ðŸ” Browser Fingerprint:")
                        print(f"  ðŸ“± User Agent: {fingerprint.get('user_agent', 'default')[:50]}...")
                        print(f"  ðŸ–¥ï¸  Viewport: {fingerprint.get('viewport', 'default')}")
                        if fingerprint.get("proxy_used"):
                            print(f"  ðŸŒ Proxy: enabled")
            else:
                print("âŒ Crawl failed!")
                print(f"ðŸš« Error: {results.get('error', 'Unknown error')}")
                if results.get('stealth_attempted'):
                    print("ðŸ›¡ï¸  Stealth mode was attempted")
                if results.get('suggestion'):
                    print(f"ðŸ’¡ Suggestion: {results['suggestion']}")
            
            # Save to file if requested
            if hasattr(args, 'output') and args.output:
                tool.save_results(results, args.output)
            else:
                # Print content preview - prioritize AI extraction for AI commands
                if "ai_extraction" in results and results["ai_extraction"]:
                    ai_content = results["ai_extraction"]
                    print(f"\nðŸ¤– AI Extraction:")
                    print("-" * 30)
                    if len(ai_content) > 500:
                        print(f"{ai_content[:500]}...")
                        print(f"\n[AI extraction truncated - {len(ai_content)} total chars]")
                    else:
                        print(ai_content)
                
                # Print regular content preview
                content = results.get('content', '') or results.get('markdown_content', '')
                if content and len(content) > 500:
                    print(f"\nðŸ“„ Content preview ({len(content)} chars):")
                    print("-" * 30)
                    print(content[:500] + "...")
                elif content:
                    print(f"\nðŸ“„ Content ({len(content)} chars):")
                    print("-" * 30)
                    print(content)
    
    except Exception as e:
        print(f"ðŸ’¥ Unexpected error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ðŸ•·ï¸  Crawl4AI POC Tool - Comprehensive Web Scraping Demonstration")
    print("=" * 60)
    asyncio.run(main())